#!/usr/bin/env python3
"""
Comprehensive Test Suite for Sonnet 4 Bot Centralized Tools System
================================================================
Tests the new centralized tools handling, interleaved thinking,
and parallel tool execution capabilities with enhanced logging.
"""

import asyncio
import logging
import json
from datetime import datetime
from sonnet4_bot import JibAI

# Configure detailed logging to see all agent steps
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class MockGlobalStorage:
    """Mock global storage for testing"""
    def __init__(self):
        self.data = {
            'packages': [],
            'categories': [],
            'brands': []
        }

async def test_centralized_tools_comprehensive():
    """Comprehensive test of the centralized tools system"""
    
    print("ğŸš€ Starting Comprehensive Centralized Tools Testing")
    print("=" * 80)
    
    # Initialize JibAI with mock storage
    mock_storage = MockGlobalStorage()
    jib_ai = JibAI(mock_storage)
    
    # Test cases covering different scenarios
    test_cases = [
        {
            "name": "Simple Greeting (RAG Context Only)",
            "query": "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š",
            "room_id": "greeting_test",
            "description": "Should add initial RAG context but not use tools",
            "expected_no_tools": True
        },
        {
            "name": "Basic Vaccine Inquiry",
            "query": "à¸¡à¸µà¸§à¸±à¸„à¸‹à¸µà¸™ HPV à¹„à¸«à¸¡ à¸£à¸²à¸„à¸²à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆ",
            "room_id": "vaccine_basic",
            "description": "Should use retrieval tool to search for HPV vaccines",
            "expected_tools": ["retrieval"]
        },
        {
            "name": "Price Filtering Query",
            "query": "à¸«à¸²à¹à¸à¹‡à¸à¹€à¸à¸ˆà¸•à¸£à¸§à¸ˆà¸ªà¸¸à¸‚à¸ à¸²à¸à¸£à¸²à¸„à¸²à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 5000 à¸šà¸²à¸— 10 à¸­à¸±à¸™à¸”à¸±à¸šà¹à¸£à¸",
            "room_id": "price_filter",
            "description": "Should use SQL search for price filtering",
            "expected_tools": ["sql_search"]
        },
        {
            "name": "Shopping Cart Creation",
            "query": "à¸Šà¹ˆà¸§à¸¢à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸°à¸à¸£à¹‰à¸²à¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸«à¹‰à¸«à¸™à¹ˆà¸­à¸¢",
            "room_id": "cart_create",
            "description": "Should use cart tool to create new cart",
            "expected_tools": ["cart"]
        },
        {
            "name": "Complex Multi-step Query",
            "query": "à¸„à¹‰à¸™à¸«à¸²à¸§à¸±à¸„à¸‹à¸µà¸™ HPV à¸£à¸²à¸„à¸²à¸–à¸¹à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸” 3 à¸­à¸±à¸™à¸”à¸±à¸š à¹à¸¥à¹‰à¸§à¹ƒà¸ªà¹ˆà¸­à¸±à¸™à¸”à¸±à¸š 1 à¸¥à¸‡à¸•à¸°à¸à¸£à¹‰à¸²",
            "room_id": "multi_step",
            "description": "Should use multiple tools: SQL search, then cart operations",
            "expected_tools": ["sql_search", "cart"]
        },
        {
            "name": "Booking with Complete Info",
            "query": "à¸œà¸¡à¸Šà¸·à¹ˆà¸­ à¸ˆà¸­à¸«à¹Œà¸™ à¸ªà¸¡à¸´à¸˜ à¹‚à¸—à¸£ 081-234-5678 à¸­à¸¢à¸²à¸à¸ˆà¸­à¸‡à¹à¸à¹‡à¸à¹€à¸à¸ˆà¸•à¸£à¸§à¸ˆà¸ªà¸¸à¸‚à¸ à¸²à¸à¸§à¸±à¸™à¸¨à¸¸à¸à¸£à¹Œà¸«à¸™à¹‰à¸² à¹€à¸§à¸¥à¸² 14:00",
            "room_id": "booking_complete",
            "description": "Should search for packages and handover to booking specialist",
            "expected_tools": ["retrieval", "handover_to_bk"]
        },
        {
            "name": "Technical Emergency",
            "query": "à¸£à¸°à¸šà¸šà¸‚à¸±à¸”à¸‚à¹‰à¸­à¸‡ à¹€à¸‚à¹‰à¸²à¹„à¸¡à¹ˆà¹„à¸”à¹‰ à¸Šà¹ˆà¸§à¸¢à¸”à¹ˆà¸§à¸™",
            "room_id": "emergency",
            "description": "Should immediately handover for urgent technical issues",
            "expected_tools": ["handover_asap"]
        },
        {
            "name": "Complex Comparison Query",
            "query": "à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹à¸à¹‡à¸à¹€à¸à¸ˆà¸—à¸±à¸™à¸•à¸à¸£à¸£à¸¡à¹ƒà¸™à¸à¸£à¸¸à¸‡à¹€à¸—à¸à¹à¸¥à¸°à¹€à¸Šà¸µà¸¢à¸‡à¹ƒà¸«à¸¡à¹ˆ à¸£à¸²à¸„à¸² 2000-8000 à¸šà¸²à¸—",
            "room_id": "comparison",
            "description": "Should use SQL search for filtering and retrieval for details",
            "expected_tools": ["sql_search", "retrieval"]
        }
    ]
    
    results_summary = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ§ª TEST {i}/{len(test_cases)}: {test_case['name']}")
        print(f"ğŸ“ Query: {test_case['query']}")
        print(f"ğŸ’¡ Expected: {test_case['description']}")
        print('='*60)
        
        try:
            # Prepare mock chat history
            chats = [{"role": "user", "content": test_case['query']}]
            
            # Run the test with timing
            start_time = datetime.now()
            result = await jib_ai.forward(chats, test_case['room_id'], test_case['query'])
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            
            # Extract results
            chat_resp, token_dict, thought_dict = result
            response_text = chat_resp.get('text', 'No response')
            tools_used = thought_dict.get('tools_used', [])
            tool_count = thought_dict.get('tool_count', 0)
            thinking_content = thought_dict.get('thinking_content', '')
            
            # Analyze results
            used_tool_names = [tool['name'] for tool in tools_used]
            expected_tools = test_case.get('expected_tools', [])
            expected_no_tools = test_case.get('expected_no_tools', False)
            
            # Determine test status
            if expected_no_tools:
                test_passed = len(tools_used) == 0
                status = "âœ… PASSED" if test_passed else "âŒ FAILED"
                reason = "No tools used as expected" if test_passed else f"Unexpected tools used: {used_tool_names}"
            elif expected_tools:
                tools_found = any(tool in used_tool_names for tool in expected_tools)
                test_passed = tools_found
                status = "âœ… PASSED" if test_passed else "âš ï¸ PARTIAL"
                reason = f"Expected tools found: {expected_tools}" if test_passed else f"Expected {expected_tools}, got {used_tool_names}"
            else:
                test_passed = True
                status = "âœ… COMPLETED"
                reason = "No specific tool expectations"
            
            # Display results
            print(f"\n{status}: {reason}")
            print(f"ğŸ“± RESPONSE: {response_text[:150]}...")
            print(f"â±ï¸  PROCESSING TIME: {processing_time:.2f}s")
            print(f"ğŸ”§ TOOLS USED ({tool_count}):")
            
            if tools_used:
                for j, tool in enumerate(tools_used, 1):
                    print(f"  {j}. {tool['name']}")
                    print(f"     ğŸ“¥ Input: {str(tool['input'])[:80]}...")
                    print(f"     ğŸ“¤ Result: {str(tool['result'])[:80]}...")
            else:
                print("  (No tools used)")
            
            # Thinking analysis
            thinking_blocks = thinking_content.split('\n\n') if thinking_content else []
            print(f"ğŸ§  THINKING BLOCKS: {len(thinking_blocks)}")
            if thinking_blocks:
                print(f"ğŸ§  FIRST BLOCK: {thinking_blocks[0][:100]}...")
            
            print(f"ğŸ“Š ESTIMATED TOKENS: {token_dict.get('total_tokens', 0)}")
            
            # Store results for summary
            results_summary.append({
                'test_name': test_case['name'],
                'status': status,
                'processing_time': processing_time,
                'tools_used': len(tools_used),
                'thinking_blocks': len(thinking_blocks),
                'passed': test_passed
            })
            
        except Exception as e:
            print(f"âŒ ERROR in test {i}: {str(e)}")
            logger.error(f"Test {i} ({test_case['name']}) failed: {e}")
            results_summary.append({
                'test_name': test_case['name'],
                'status': 'âŒ ERROR',
                'processing_time': 0,
                'tools_used': 0,
                'thinking_blocks': 0,
                'passed': False,
                'error': str(e)
            })
        
        print(f"\n{'-'*40}")
    
    # Final summary
    print(f"\n{'='*60}")
    print("ğŸ“Š TESTING SUMMARY")
    print('='*60)
    
    passed_tests = sum(1 for r in results_summary if r['passed'])
    total_tests = len(results_summary)
    avg_processing_time = sum(r['processing_time'] for r in results_summary) / len(results_summary)
    total_tools_used = sum(r['tools_used'] for r in results_summary)
    total_thinking_blocks = sum(r['thinking_blocks'] for r in results_summary)
    
    print(f"âœ… PASSED: {passed_tests}/{total_tests} tests")
    print(f"â±ï¸  AVERAGE PROCESSING TIME: {avg_processing_time:.2f}s")
    print(f"ğŸ”§ TOTAL TOOLS EXECUTED: {total_tools_used}")
    print(f"ğŸ§  TOTAL THINKING BLOCKS: {total_thinking_blocks}")
    
    print(f"\nğŸ“‹ DETAILED RESULTS:")
    for result in results_summary:
        print(f"  {result['status']} {result['test_name']} ({result['processing_time']:.2f}s, {result['tools_used']} tools)")
    
    print(f"\nğŸ Centralized Tools Testing Completed!")
    print("ğŸ’¡ Check the detailed logs above to see agent's step-by-step reasoning!")

async def test_parallel_execution():
    """Test parallel tool execution capabilities"""
    print(f"\n{'='*60}")
    print("âš¡ TESTING PARALLEL TOOL EXECUTION")
    print('='*60)
    
    mock_storage = MockGlobalStorage()
    jib_ai = JibAI(mock_storage)
    
    # Query designed to potentially trigger multiple tools
    parallel_query = "à¸„à¹‰à¸™à¸«à¸²à¸§à¸±à¸„à¸‹à¸µà¸™à¸£à¸²à¸„à¸²à¸–à¸¹à¸ à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸°à¸à¸£à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ à¹à¸¥à¸°à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸­à¸‡"
    
    print(f"ğŸ“ Parallel Test Query: {parallel_query}")
    
    try:
        chats = [{"role": "user", "content": parallel_query}]
        
        start_time = datetime.now()
        result = await jib_ai.forward(chats, "parallel_test", parallel_query)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        chat_resp, token_dict, thought_dict = result
        tools_used = thought_dict.get('tools_used', [])
        
        print(f"\nâœ… PARALLEL TEST RESULTS:")
        print(f"â±ï¸  Processing Time: {processing_time:.2f}s")
        print(f"ğŸ”§ Tools Executed: {len(tools_used)}")
        
        if len(tools_used) > 1:
            print(f"âš¡ Multiple tools executed - parallel capability demonstrated")
            for i, tool in enumerate(tools_used, 1):
                print(f"  {i}. {tool['name']}")
        elif len(tools_used) == 1:
            print(f"ğŸ”„ Single tool executed: {tools_used[0]['name']}")
        else:
            print(f"ğŸ’¬ No tools used - direct response provided")
            
        print(f"ğŸ“± Response: {chat_resp.get('text', '')[:100]}...")
        
    except Exception as e:
        print(f"âŒ Parallel test error: {e}")

if __name__ == "__main__":
    print("ğŸ¯ SONNET 4 BOT - CENTRALIZED TOOLS TESTING SUITE")
    print(f"ğŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ” Watch the logs for detailed agent reasoning and tool execution steps!\n")
    
    # Run comprehensive tests
    asyncio.run(test_centralized_tools_comprehensive())
    
    # Run parallel execution test
    asyncio.run(test_parallel_execution())
    
    print(f"\nğŸ“… Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("âœ¨ All tests completed! Review the detailed logs above for agent insights.") 